{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>第9章: ベクトル空間法</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enwiki-20150112-400-r10-105752.txt.bz2は，2015年1月12日時点の英語のWikipedia記事のうち，約400語以上で構成される記事の中から，ランダムに1/10サンプリングした105,752記事のテキストをbzip2形式で圧縮したものである．<br>\n",
    "このテキストをコーパスとして，単語の意味を表すベクトル（分散表現）を学習したい．<br>\n",
    "第9章の前半では，コーパスから作成した単語文脈共起行列に主成分分析を適用し，単語ベクトルを学習する過程を，いくつかの処理に分けて実装する．<br>\n",
    "第9章の後半では，学習で得られた単語ベクトル（300次元）を用い，単語の類似度計算やアナロジー（類推）を行う．<br>\n",
    "\n",
    "なお，問題83を素直に実装すると，大量（約7GB）の主記憶が必要になる． <br>\n",
    "メモリが不足する場合は，処理を工夫するか，1/100サンプリングのコーパスenwiki-20150112-400-r100-10576.txt.bz2を用いよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>目次</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [80. コーパスの整形](#prob80)\n",
    "- [81. 複合語からなる国名への対処](#prob81)\n",
    "- [82. 文脈の抽出](#prob82)\n",
    "- [83. 単語/文脈の頻度の計測](#prob83)\n",
    "- [84. 単語文脈行列の作成](#prob84)\n",
    "- [85. 主成分分析による次元圧縮](#prob85)\n",
    "- [86. 単語ベクトルの表示](#prob86)\n",
    "- [87. 単語の類似度](#prob87)\n",
    "- [88. 類似度の高い単語10件](#prob88)\n",
    "- [89. 加法構成性によるアナロジー](#prob89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**とりあえずコンパクトにやりたいから1/100コーパス使う**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-08-22 14:46:48--  http://www.cl.ecei.tohoku.ac.jp/nlp100/data/enwiki-20150112-400-r100-10576.txt.bz2\n",
      "www.cl.ecei.tohoku.ac.jp (www.cl.ecei.tohoku.ac.jp) をDNSに問いあわせています... 130.34.192.83\n",
      "www.cl.ecei.tohoku.ac.jp (www.cl.ecei.tohoku.ac.jp)|130.34.192.83|:80 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 22022468 (21M) [application/x-bzip2]\n",
      "`./data/enwiki-20150112-400-r100-10576.txt.bz2' に保存中\n",
      "\n",
      "enwiki-20150112-400 100%[===================>]  21.00M  3.52MB/s 時間 5.0s     \n",
      "\n",
      "2018-08-22 14:46:54 (4.23 MB/s) - `./data/enwiki-20150112-400-r100-10576.txt.bz2' へ保存完了 [22022468/22022468]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -NP ./data/ http://www.cl.ecei.tohoku.ac.jp/nlp100/data/enwiki-20150112-400-r100-10576.txt.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bzip2, a block-sorting file compressor.  Version 1.0.6, 6-Sept-2010.\n",
      "\n",
      "   usage: bunzip2 [flags and input files in any order]\n",
      "\n",
      "   -h --help           print this message\n",
      "   -d --decompress     force decompression\n",
      "   -z --compress       force compression\n",
      "   -k --keep           keep (don't delete) input files\n",
      "   -f --force          overwrite existing output files\n",
      "   -t --test           test compressed file integrity\n",
      "   -c --stdout         output to standard out\n",
      "   -q --quiet          suppress noncritical error messages\n",
      "   -v --verbose        be verbose (a 2nd -v gives more)\n",
      "   -L --license        display software version & license\n",
      "   -V --version        display software version & license\n",
      "   -s --small          use less memory (at most 2500k)\n",
      "   -1 .. -9            set block size to 100k .. 900k\n",
      "   --fast              alias for -1\n",
      "   --best              alias for -9\n",
      "\n",
      "   If invoked as `bzip2', default action is to compress.\n",
      "              as `bunzip2',  default action is to decompress.\n",
      "              as `bzcat', default action is to decompress to stdout.\n",
      "\n",
      "   If no file names are given, bzip2 compresses or decompresses\n",
      "   from standard input to standard output.  You can combine\n",
      "   short flags, so `-v -4' means the same as -v4 or -4v, &c.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!bunzip2 --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bunzip2 -k ./data/enwiki-20150112-400-r100-10576.txt.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism\n",
      "\n",
      "Anarchism is a political philosophy that advocates stateless societies often defined as self-governed voluntary institutions, but that several authors have defined as more specific institutions based on non-hierarchical free associations. Anarchism holds the state to be undesirable, unnecessary, or harmful. While anti-statism is central, anarchism entails opposing authority or hierarchical organisation in the conduct of human relations, including, but not limited to, the state system.\n",
      "As a subtle and anti-dogmatic philosophy, anarchism draws on many currents of thought and strategy. Anarchism does not offer a fixed body of doctrine from a single particular world view, instead fluxing and flowing as a philosophy. There are many types and traditions of anarchism, not all of which are mutually exclusive. Anarchist schools of thought can differ fundamentally, supporting anything from extreme individualism to complete collectivism. Strains of anarchism have often been divided into the categories of social and individualist anarchism or similar dual classifications. Anarchism is usually considered a radical left-wing ideology, and much of anarchist economics and anarchist legal philosophy reflect anti-authoritarian interpretations of communism, collectivism, syndicalism, mutualism, or participatory economics.\n",
      "The central tendency of anarchism as a social movement has been represented by anarcho-communism and anarcho-syndicalism, with individualist anarchism being primarily a literary phenomenon which nevertheless did have an impact on the bigger currents and individualists have also participated in large anarchist organisations. Many anarchists oppose all forms of aggression, supporting self-defense or non-violence (anarcho-pacifism), while others have supported the use of some coercive measures, including violent revolution and propaganda of the deed as means to achieve anarchist ends.\n",
      "Etymology and terminology.\n",
      "The term \"anarchism\" is a compound word composed from the word \"anarchy\" and the suffix \"-ism\", themselves derived respectively from the Greek , i.e. \"anarchy\" (from , \"anarchos\", meaning \"one without rulers\"; from the privative prefix ἀν- (\"an-\", i.e. \"without\") and , \"archos\", i.e. \"leader\", \"ruler\"; (cf. \"archon\" or , \"arkhē\", i.e. \"authority\", \"sovereignty\", \"realm\", \"magistracy\")) and the suffix or (\"-ismos\", \"-isma\", from the verbal infinitive suffix -ίζειν, \"-izein\"). The first known use of this word was in 1539.\"Anarchist\" was the term adopted by Maximilien de Robespierre to attack those on the left whom he had used for his own ends during the French Revolution but was determined to get rid of, though among these \"anarchists\" there were few who exhibited the social revolt characteristics of later anarchists. There would be many revolutionaries of the early nineteenth century who contributed to the anarchist doctrines of the next generation, such as William Godwin and Wilhelm Weitling, but they did not use the word \"anarchist\" or \"anarchism\" in describing themselves or their beliefs. Pierre-Joseph Proudhon was the first political philosopher to call himself an anarchist, marking the formal birth of anarchism in the mid-nineteenth century. Since the 1890s from France, the term \"libertarianism\" has often been used as a synonym for anarchism and was used almost exclusively in this sense until the 1950s in the United States; its use as a synonym is still common outside the United States. On the other hand, some use \"libertarianism\" to refer to individualistic free-market philosophy only, referring to free-market anarchism as \"libertarian anarchism\".\n",
      "History.\n",
      "Origins.\n",
      "The earliest anarchist themes can be found in the 6th century BC, among the works of Taoist philosopher Laozi, and in later centuries by Zhuangzi and Bao Jingyan. Zhuangzi's philosophy has been described by various sources as anarchist. Zhuangzi wrote, \"A petty thief is put in jail. A great brigand becomes a ruler of a Nation.\" Diogenes of Sinope and the Cynics, their contemporary Zeno of Citium, the founder of Stoicism, also introduced similar topics. Jesus is sometimes considered the first anarchist in the Christian anarchist tradition. Georges Lechartier wrote that \"The true founder of anarchy was Jesus Christ and ... the first anarchist society was that of the apostles.\" In early Islamic history, some manifestations of anarchic thought are found during the Islamic civil war over the Caliphate, where the Kharijites insisted that the imamate is a right for each individual within the Islamic society. Later, some Muslim scholars, such as Amer al-Basri and Abu Hanifa, led movements of boycotting the rulers, paving the way to the waqf (endowments) tradition, which served as an alternative to and asylum from the centralized authorities of the emirs. But such interpretations reverberates subversive religious conceptions like the aforementioned seemingly anarchistic Taoist teachings and that of other anti-authoritarian religious traditions creating a complex relationship regarding the question as to whether or not anarchism and religion are compatible. This is exemplified when the glorification of the state is viewed as a form of sinful idolatry.\n"
     ]
    }
   ],
   "source": [
    "!cat ./data/enwiki-20150112-400-r100-10576.txt 2>/dev/null | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='prob80'></a>\n",
    "# 80. コーパスの整形"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文を単語列に変換する最も単純な方法は，空白文字で単語に区切ることである． <br>\n",
    "ただ，この方法では文末のピリオドや括弧などの記号が単語に含まれてしまう． <br>\n",
    "そこで，コーパスの各行のテキストを空白文字でトークンのリストに分割した後，各トークンに以下の処理を施し，単語から記号を除去せよ．<br>\n",
    "\n",
    "トークンの先頭と末尾に出現する次の文字を削除: .,!?;:()[]'\"<br>\n",
    "空文字列となったトークンは削除<br>\n",
    "以上の処理を適用した後，トークンをスペースで連結してファイルに保存せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/enwiki-20150112-400-r100-10576.txt') as f, open('./work/text_tokenized.txt', 'w') as out:\n",
    "    for line in f:\n",
    "        tokens = line.rstrip().split()\n",
    "        print(*(token.strip('.,!?;:()[]\\'\"') for token in tokens if len(token.strip('.,!?;:()[]\\'\"')) > 0), file=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token.rstrip...って2回書きたくないけど1トークンごとに書き込みたくなかった + もう1つ配列用意してappendするのも効率悪そう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='prob81'></a>\n",
    "# 81. 複合語からなる国名への対処"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "英語では，複数の語の連接が意味を成すことがある．<br>\n",
    "例えば，アメリカ合衆国は\"United States\"，イギリスは\"United Kingdom\"と表現されるが，\"United\"や\"States\"，\"Kingdom\"という単語だけでは，指し示している概念・実体が曖昧である．<br>\n",
    "そこで，コーパス中に含まれる複合語を認識し，複合語を1語として扱うことで，複合語の意味を推定したい．<br>\n",
    "しかしながら，複合語を正確に認定するのは大変むずかしいので，ここでは複合語からなる国名を認定したい．<br>\n",
    "\n",
    "インターネット上から国名リストを各自で入手し，80のコーパス中に出現する複合語の国名に関して，スペースをアンダーバーに置換せよ．<br>\n",
    "例えば，\"United States\"は\"United_States\"，\"Isle of Man\"は\"Isle_of_Man\"になるはずである．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>pandasがHTMLの\\<table>タグのスクレイピングにクッソ便利な件</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.io.html.read_html('https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_and_their_capitals_in_native_languages',\n",
    "                         skiprows=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Kabul</td>\n",
       "      <td>Afghanestanافغانستان</td>\n",
       "      <td>Kabulكابل</td>\n",
       "      <td>Pashto/Dari(Arabic script)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Tirana</td>\n",
       "      <td>Shqipëria</td>\n",
       "      <td>Tirana</td>\n",
       "      <td>Albanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>Algiers</td>\n",
       "      <td>DzayerⴷⵣⴰⵢⴻⵔAl-Jazā'irالجزائر</td>\n",
       "      <td>DzayerⴷⵣⴰⵢⴻⵔAl-Jazā'irالجزائر</td>\n",
       "      <td>Berber language(Tifinagh script)Arabic(Arabic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>American Samoa[1]</td>\n",
       "      <td>Pago Pago</td>\n",
       "      <td>Amerika SāmoaAmerican Samoa</td>\n",
       "      <td>Pago Pago Pago Pago</td>\n",
       "      <td>SamoanEnglish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>Andorra la Vella</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>Andorra la Vella</td>\n",
       "      <td>Catalan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Angola</td>\n",
       "      <td>Luanda</td>\n",
       "      <td>Angola Ngola</td>\n",
       "      <td>Luanda Lwanda</td>\n",
       "      <td>Portuguese Kongo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Anguilla[2]</td>\n",
       "      <td>The Valley</td>\n",
       "      <td>Anguilla</td>\n",
       "      <td>The Valley</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>Saint John's</td>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>St. John's</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>Buenos Aires</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Buenos Aires</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>Yerevan</td>\n",
       "      <td>HayastánՀայաստան</td>\n",
       "      <td>YerevanԵրեվան</td>\n",
       "      <td>Armenian(Armenian alphabet)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Aruba[3]</td>\n",
       "      <td>Oranjestad</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>Oranjestad</td>\n",
       "      <td>Dutch, Papiamento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Australia</td>\n",
       "      <td>Canberra</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Canberra</td>\n",
       "      <td>English/ Aboriginal native languages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Austria</td>\n",
       "      <td>Vienna</td>\n",
       "      <td>Österreich</td>\n",
       "      <td>Wien</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Åland Islands[4]</td>\n",
       "      <td>Mariehamn</td>\n",
       "      <td>ÅlandAhvenanmaa</td>\n",
       "      <td>MariehamnMaarianhamina</td>\n",
       "      <td>SwedishFinnish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>Baku</td>\n",
       "      <td>Azərbaycan</td>\n",
       "      <td>Bakı</td>\n",
       "      <td>Azerbaijani</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0                 1                              2  \\\n",
       "0           Afghanistan             Kabul           Afghanestanافغانستان   \n",
       "1               Albania            Tirana                      Shqipëria   \n",
       "2               Algeria           Algiers  DzayerⴷⵣⴰⵢⴻⵔAl-Jazā'irالجزائر   \n",
       "3     American Samoa[1]         Pago Pago    Amerika SāmoaAmerican Samoa   \n",
       "4               Andorra  Andorra la Vella                        Andorra   \n",
       "5                Angola            Luanda                   Angola Ngola   \n",
       "6           Anguilla[2]        The Valley                       Anguilla   \n",
       "7   Antigua and Barbuda      Saint John's            Antigua and Barbuda   \n",
       "8             Argentina      Buenos Aires                      Argentina   \n",
       "9               Armenia           Yerevan               HayastánՀայաստան   \n",
       "10             Aruba[3]        Oranjestad                          Aruba   \n",
       "11            Australia          Canberra                      Australia   \n",
       "12              Austria            Vienna                     Österreich   \n",
       "13     Åland Islands[4]         Mariehamn                ÅlandAhvenanmaa   \n",
       "14           Azerbaijan              Baku                     Azərbaycan   \n",
       "\n",
       "                                3  \\\n",
       "0                       Kabulكابل   \n",
       "1                          Tirana   \n",
       "2   DzayerⴷⵣⴰⵢⴻⵔAl-Jazā'irالجزائر   \n",
       "3             Pago Pago Pago Pago   \n",
       "4                Andorra la Vella   \n",
       "5                   Luanda Lwanda   \n",
       "6                      The Valley   \n",
       "7                      St. John's   \n",
       "8                    Buenos Aires   \n",
       "9                   YerevanԵրեվան   \n",
       "10                     Oranjestad   \n",
       "11                       Canberra   \n",
       "12                           Wien   \n",
       "13         MariehamnMaarianhamina   \n",
       "14                           Bakı   \n",
       "\n",
       "                                                    4  \n",
       "0                          Pashto/Dari(Arabic script)  \n",
       "1                                            Albanian  \n",
       "2   Berber language(Tifinagh script)Arabic(Arabic ...  \n",
       "3                                       SamoanEnglish  \n",
       "4                                             Catalan  \n",
       "5                                    Portuguese Kongo  \n",
       "6                                             English  \n",
       "7                                             English  \n",
       "8                                             Spanish  \n",
       "9                         Armenian(Armenian alphabet)  \n",
       "10                                  Dutch, Papiamento  \n",
       "11               English/ Aboriginal native languages  \n",
       "12                                             German  \n",
       "13                                     SwedishFinnish  \n",
       "14                                        Azerbaijani  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Xから始まる国はないという罠 (最初range(26)で回してた)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = [re.sub('\\[[0-9]+\\]', '', df[table_num][0][row]) for table_num in range(25) for row in range(len(df[table_num]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Afghanistan',\n",
       " 'Albania',\n",
       " 'Algeria',\n",
       " 'American Samoa',\n",
       " 'Andorra',\n",
       " 'Angola',\n",
       " 'Anguilla',\n",
       " 'Antigua and Barbuda',\n",
       " 'Argentina',\n",
       " 'Armenia']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "()がじゃまなやつとかあったからそれも削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_link = re.compile('\\[[0-9]+\\]')\n",
    "regex_paren = re.compile('\\s\\(.+\\)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = [regex_paren.sub('', regex_link.sub('', df[table_num][0][row])) for table_num in range(25) for row in range(len(df[table_num]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Afghanistan',\n",
       " 'Albania',\n",
       " 'Algeria',\n",
       " 'American Samoa',\n",
       " 'Andorra',\n",
       " 'Angola',\n",
       " 'Anguilla',\n",
       " 'Antigua and Barbuda',\n",
       " 'Argentina',\n",
       " 'Armenia']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_comp = set(country for country in countries if ' ' in country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'American Samoa',\n",
       " 'Antigua and Barbuda',\n",
       " 'Bosnia and Herzegovina',\n",
       " 'British Virgin Islands',\n",
       " 'Burkina Faso',\n",
       " 'Cape Verde',\n",
       " 'Cayman Islands',\n",
       " 'Central African Republic',\n",
       " 'Christmas Island',\n",
       " 'Cocos Islands',\n",
       " 'Cook Islands',\n",
       " 'Costa Rica',\n",
       " 'Czech Republic',\n",
       " \"Côte d'Ivoire(formerly Ivory Coast)\",\n",
       " 'Democratic Republic of the Congo',\n",
       " 'Dominican Republic',\n",
       " 'East Timor',\n",
       " 'El Salvador',\n",
       " 'Equatorial Guinea',\n",
       " 'Eswatini(formerly Swaziland)',\n",
       " 'Falkland Islands',\n",
       " 'Faroe Islands',\n",
       " 'Federated States of Micronesia',\n",
       " 'French Guiana',\n",
       " 'French Polynesia',\n",
       " 'Hong Kong',\n",
       " 'Isle of Man',\n",
       " 'Marshall Islands',\n",
       " 'New Caledonia',\n",
       " 'New Zealand',\n",
       " 'Norfolk Island',\n",
       " 'North Korea',\n",
       " 'Northern Mariana Islands',\n",
       " 'Papua New Guinea',\n",
       " 'Pitcairn Islands',\n",
       " 'Puerto Rico',\n",
       " 'Republic of the Congo',\n",
       " 'Sahrawi Arab Democratic Republic',\n",
       " 'Saint Barthélemy',\n",
       " 'Saint Helena, Ascension and Tristan da Cunha',\n",
       " 'Saint Kitts and Nevis',\n",
       " 'Saint Lucia',\n",
       " 'Saint Martin',\n",
       " 'Saint Pierre and Miquelon',\n",
       " 'Saint Vincent and the Grenadines',\n",
       " 'San Marino',\n",
       " 'Saudi Arabia',\n",
       " 'Sierra Leone',\n",
       " 'Sint Maarten',\n",
       " 'Solomon Islands',\n",
       " 'South Africa',\n",
       " 'South Korea',\n",
       " 'South Sudan',\n",
       " 'Sri Lanka',\n",
       " 'São Tomé and Príncipe',\n",
       " 'The Bahamas',\n",
       " 'The Gambia',\n",
       " 'Trinidad and Tobago',\n",
       " 'Turks and Caicos Islands',\n",
       " 'United Arab Emirates',\n",
       " 'United Kingdom',\n",
       " 'United States',\n",
       " 'United States Virgin Islands',\n",
       " 'Vatican City',\n",
       " 'Wallis and Futuna',\n",
       " 'Åland Islands'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('work/text_tokenized.txt') as f, open('./work/text_tokenized_comp.txt', 'w') as out:\n",
    "    for line in f:\n",
    "        for country in sorted(countries_comp, key=lambda x: len(x), reverse=True):\n",
    "            line = line.replace(country, '_'.join(country.split()))\n",
    "        print(line.rstrip(), file=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "長い文から順に見ていくと国名の一部がかぶっている場合に最長一致を見てこられる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='prob82'></a>\n",
    "# 82. 文脈の抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "81で作成したコーパス中に出現するすべての単語$t$に関して，単語$t$と文脈語$c$のペアをタブ区切り形式ですべて書き出せ．<br>\n",
    "ただし，文脈語の定義は次の通りとする．<br>\n",
    "\n",
    "- ある単語$t$の前後$d$単語を文脈語$c$として抽出する（ただし，文脈語に単語$t$そのものは含まない）\n",
    "- 単語$t$を選ぶ度に，文脈幅$d$は$\\{1,2,3,4,5\\}$の範囲でランダムに決める．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./work/text_tokenized_comp.txt') as f, open('./work/context.txt' ,'w') as out:\n",
    "    for line in f:\n",
    "        words = line.rstrip().split()\n",
    "        for idx, word in enumerate(words):\n",
    "            window = random.randint(1,5)\n",
    "            context = [words[idx+i] for i in range(-window,window+1) if 0 <= idx+i < len(words) and i != 0]\n",
    "            if len(context) > 0:\n",
    "                for c in context:\n",
    "                    print(word, c, sep='\\t', file=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism\tis\n",
      "Anarchism\ta\n",
      "Anarchism\tpolitical\n",
      "Anarchism\tphilosophy\n",
      "is\tAnarchism\n",
      "is\ta\n",
      "a\tAnarchism\n",
      "a\tis\n",
      "a\tpolitical\n",
      "a\tphilosophy\n"
     ]
    }
   ],
   "source": [
    "!head ./work/context.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68091560 ./work/context.txt\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./work/context.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='prob83'></a>\n",
    "# 83. 単語／文脈の頻度の計測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "82の出力を利用し，以下の出現分布，および定数を求めよ．<br>\n",
    "\n",
    "$f(t,c)$: 単語$t$と文脈語$c$の共起回数<br>\n",
    "$f(t,∗)$: 単語$t$の出現回数<br>\n",
    "$f(∗,c)$: 文脈語$c$の出現回数<br>\n",
    "$N$: 単語と文脈語のペアの総出現回数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**この実装は1行に注目単語と複数のcontextが書かれている出力についてのものになっている (tが複数回カウントされることを考える必要がない)**\n",
    "\n",
    "(82でfor文使わないで`print(word, *context, sep='\\t', file=out)`する)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_tc = Counter()\n",
    "f_t = Counter()\n",
    "f_c = Counter()\n",
    "N = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./work/context.txt') as f:\n",
    "    for line in f:\n",
    "        word, contexts = line.rstrip().split('\\t', maxsplit=1)\n",
    "        f_t.update({word:1})\n",
    "        for context in contexts.split('\\t'):\n",
    "            f_c.update({context:1})\n",
    "            f_tc.update({(word, context):1})\n",
    "            N += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 754586),\n",
       " ('of', 410555),\n",
       " ('and', 357978),\n",
       " ('in', 283554),\n",
       " ('to', 262920),\n",
       " ('a', 234498),\n",
       " ('was', 137710),\n",
       " ('The', 119810),\n",
       " ('is', 106471),\n",
       " ('for', 98698)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_t.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('the', 'of'), 282255),\n",
       " (('of', 'the'), 282089),\n",
       " (('the', 'the'), 157765),\n",
       " (('the', 'in'), 129301),\n",
       " (('in', 'the'), 129221),\n",
       " (('the', 'and'), 115802),\n",
       " (('and', 'the'), 115350),\n",
       " (('the', 'to'), 107403),\n",
       " (('to', 'the'), 106905),\n",
       " (('and', 'of'), 57127)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_tc.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ループ毎回回すの非効率だからpickleでシリアライズして保存しとく(**jsonでよくね?**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./work/variables.pickle', 'wb') as out:\n",
    "    pickle.dump({'f_tc':f_tc, 'f_t':f_t, 'f_c':f_c, 'N':N}, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tに当たる単語をソートして逐次f_t, f_tcを出力していけば良い\n",
    "\n",
    "ファイルを分割してそれぞれtでソートした上でマージソート的にくっつけるなど"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='prob84'></a>\n",
    "# 84. 単語文脈行列の作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "83の出力を利用し，単語文脈行列$X$を作成せよ．<br>\n",
    "ただし，行列$X$の各要素$X_{tc}$は次のように定義する．<br>\n",
    "\n",
    "- $f(t,c)≥10$ならば，$X_{tc}=PPMI(t,c)=\\max\\{\\log \\frac{N×f(t,c)}{f(t,∗)×f(∗,c)},0\\}$\n",
    "- $f(t,c)<10$ならば，$X_{tc}=0$<br>\n",
    "ここで，$PPMI(t,c)$はPositive Pointwise Mutual Information（正の相互情報量）と呼ばれる統計量である．<br>\n",
    "なお，行列$X$の行数・列数は数百万オーダとなり，行列のすべての要素を主記憶上に載せることは無理なので注意すること．<br>\n",
    "幸い，行列$X$のほとんどの要素は$0$になるので，非$0$の要素だけを書き出せばよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./work/variables.pickle', 'rb') as f:\n",
    "    dic = pickle.load(f)\n",
    "    f_tc = dic.get('f_tc')\n",
    "    f_t = dic.get('f_t')\n",
    "    f_c = dic.get('f_c')\n",
    "    N = dic.get('N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse, io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sparse.lil_matrix((len(f_t), len(f_c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<383369x383369 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in LInked List format>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calc_ppmi(t, c, f_tc:dict, f_t:dict, f_c:dict, N):\n",
    "    num = N * f_tc.get((t,c))\n",
    "    deno = f_t.get(t) * f_c.get(c)\n",
    "    pmi = math.log2(num / deno)\n",
    "    return max(pmi, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenからindexへの辞書"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx_t = {w:i for i, (w, _) in enumerate(f_t.items())}\n",
    "word2idx_c = {w:i for i, (w, _) in enumerate(f_c.items())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "後々必要になったので逆も"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word_t = {i:w for i, (w, _) in enumerate(f_t.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (t, c), count in f_tc.items():\n",
    "    if count >= 10:\n",
    "        idx_t = word2idx_t[t]\n",
    "        idx_c = word2idx_c[c]\n",
    "        X[idx_t, idx_c] = calc_ppmi(t, c, f_tc, f_t, f_c, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<383369x383369 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 632691 stored elements in LInked List format>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [1.38586242, 3.3644911 , 1.54082702, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [3.39695674, 1.48369332, 3.00200609, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.savemat('./work/matX', {'X':X})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='prob85'></a>\n",
    "# 85. 主成分分析による次元圧縮"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "84で得られた単語文脈行列に対して，主成分分析を適用し，単語の意味ベクトルを300次元に圧縮せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = io.loadmat('./work/matX.mat')['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PCA does not support sparse input. See TruncatedSVD for a possible alternative.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ecdad03da959>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \"\"\"\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;31m# This is more informative than the generic one raised by check_array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             raise TypeError('PCA does not support sparse input. See '\n\u001b[0m\u001b[1;32m    367\u001b[0m                             'TruncatedSVD for a possible alternative.')\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: PCA does not support sparse input. See TruncatedSVD for a possible alternative."
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=300)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCAじゃなくてTruncatedSVD使えって"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=300, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = TruncatedSVD(n_components=300)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='prob86'></a>\n",
    "# 86. 単語ベクトルの表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "85で得た単語の意味ベクトルを読み込み，\"United States\"のベクトルを表示せよ．<br>\n",
    "ただし，\"United States\"は内部的には\"United_States\"と表現されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(383369, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.45108459e+01 -3.96679861e+00 -9.18404761e+00  1.60167891e+00\n",
      "  9.56103481e+00 -1.14245349e+00 -1.58367888e+01 -8.63907950e+00\n",
      " -1.51435603e+00  5.07648537e-01 -5.50641693e-01 -8.57936355e+00\n",
      "  1.77031272e+00  9.74590868e-02  8.09741230e+00  2.13172530e+00\n",
      " -7.51825284e-01  8.57514615e-01  9.66519170e-02 -6.06572566e+00\n",
      "  1.97906481e+00  6.58550744e+00 -1.68446192e+00  6.14908757e-01\n",
      " -5.30749590e+00 -8.67372735e-01  3.56654361e+00 -8.12828922e+00\n",
      " -2.03155094e+00  2.62649428e+00 -2.83809096e+00  5.83318270e+00\n",
      "  5.64428642e-01  7.61583400e-01  8.68881950e+00 -1.14069945e+00\n",
      " -7.27356852e+00 -3.81588142e+00 -3.92720729e+00  1.59670589e+00\n",
      " -1.03880298e+00  4.50792676e-01 -5.75155190e+00  8.82337473e+00\n",
      "  8.52334128e-01  3.90851358e+00  2.20687669e+00  5.85969696e+00\n",
      " -1.56461942e-01 -1.21757910e+00 -6.18491797e-02  7.74413894e-01\n",
      "  4.62218911e+00  4.58044202e-01  6.17849536e-01  3.48293679e+00\n",
      "  2.69087197e+00  2.32873355e+00  4.84194081e-01 -1.42597133e+00\n",
      " -1.15087354e+00 -7.15916364e+00 -5.28968124e+00 -7.19628814e+00\n",
      "  7.25408079e-01  2.37896076e+00 -5.80582745e+00 -8.42415287e+00\n",
      "  8.56777628e-01 -6.91044750e+00  4.91626718e+00 -7.30725732e+00\n",
      " -3.96418360e+00 -3.78805361e+00  1.23478379e+00 -2.18878451e+00\n",
      " -2.12924091e+00 -6.49088800e+00  3.22360608e+00 -2.57016957e+00\n",
      " -2.91592401e+00  1.99801952e+00 -1.04441142e+00  4.65821350e+00\n",
      "  8.96488834e-01 -9.44412398e-01  1.66696640e-02 -2.78828993e+00\n",
      " -1.59990300e-01  3.19534664e+00 -3.55888220e+00  6.81771959e+00\n",
      "  1.88357223e+00  2.98395458e-01 -1.40977355e+00  5.87596550e+00\n",
      " -6.31397325e-02  6.38892893e+00  9.34861128e-01 -7.62275011e-02\n",
      " -3.86960525e+00 -5.99704539e+00 -6.57093483e+00 -4.65757027e+00\n",
      " -6.70446520e-01 -7.70103475e-01 -2.93186110e+00 -6.21429446e-01\n",
      "  8.28043140e-03 -3.93763582e+00  3.31593675e+00  3.60063055e+00\n",
      " -5.49257345e-02  1.08975676e+00 -2.45638404e+00 -1.14128500e-01\n",
      "  2.46110638e-01  4.37847791e+00  2.11289775e+00  6.09368180e-01\n",
      "  6.07814938e-01  2.58953785e+00  1.23009928e+00  1.73075776e+00\n",
      "  1.16726247e+00 -6.64646812e-01 -2.45712516e+00  4.57513524e+00\n",
      " -2.67511022e+00 -1.61636848e+00 -1.26196125e-01  6.39397403e-01\n",
      " -1.59654554e+00 -2.13690951e+00  8.93793759e-01 -4.64358152e-01\n",
      " -7.41403897e-01 -1.37176410e+00  5.03479965e+00  2.11637116e+00\n",
      "  1.08761928e+00 -2.90965019e-02 -1.49069067e+00 -1.20744877e+00\n",
      "  3.12992188e+00  3.91768507e-01  1.65307504e+00 -3.71906634e+00\n",
      " -1.49267403e+00  5.33613886e+00  2.07852029e+00  3.27154510e+00\n",
      " -8.98019162e-01 -6.56781791e-01 -9.17504331e-02 -5.31945746e-01\n",
      "  3.97031236e+00 -1.45588078e+00  1.65522506e+00 -4.74131752e+00\n",
      " -2.59512920e-01 -1.33288898e+00  2.70042230e+00 -3.36523988e+00\n",
      " -1.75703442e+00 -4.27894651e+00  5.65332542e-01 -1.96717869e+00\n",
      " -1.87252720e+00  1.91609872e+00 -1.11687669e+00  1.81979101e+00\n",
      "  1.14911252e+00 -3.18333159e+00 -2.62645943e-01 -8.98390913e-01\n",
      " -5.41140084e+00 -4.39262844e-01  4.74851598e-01  2.09552632e+00\n",
      "  7.54409504e-01  6.68031215e+00  4.26518234e+00  1.22830445e+00\n",
      " -7.34167246e+00  1.24074050e+00 -7.83924037e+00  2.30735267e+00\n",
      " -8.32668893e-01 -1.58187141e+00 -3.76873527e+00 -1.13260689e-01\n",
      "  1.02522630e-01  2.85547014e-03  2.17946974e+00  2.00679419e+00\n",
      " -3.68234910e+00 -7.47895986e-01 -4.42227980e+00  2.39375004e-01\n",
      " -1.81315989e+00 -1.16648777e+00 -3.82340100e-01  2.69387703e+00\n",
      "  3.06130181e+00  3.18729233e+00 -1.92193787e+00  6.10246530e-02\n",
      "  9.56841033e-01 -2.01403789e+00  7.97004249e-01  3.67543648e+00\n",
      " -3.93510626e+00  6.32696331e+00  7.06748518e-01  2.66936287e+00\n",
      " -4.45447990e+00 -3.54556741e+00  1.35962152e+00 -9.41539011e-01\n",
      "  3.83448404e+00  5.88945990e-02 -1.42446085e+00  2.41775408e+00\n",
      " -6.25741121e-01  4.02125179e-01 -1.03564837e+00 -1.32637235e+00\n",
      " -7.41606084e-01 -2.54383621e+00 -4.33958630e+00 -9.38442684e-01\n",
      " -6.06538058e-01 -4.01602452e-01  2.61020216e+00 -1.66914955e-01\n",
      "  2.31343553e+00 -3.28405147e+00  2.40823914e+00  2.14070460e+00\n",
      " -1.12136022e+00  3.48146371e+00  1.86846010e+00  2.59337173e+00\n",
      " -3.96215016e+00 -5.37578220e+00  2.22002761e+00  1.07124336e+00\n",
      "  9.44002171e-01 -1.05606003e+00 -2.01730009e+00  1.03891449e-01\n",
      "  1.17691066e+00 -1.57295710e-01  4.20247036e-01 -1.07113493e+00\n",
      "  2.18303256e+00  3.48458559e-01  1.41543611e+00  4.24662382e+00\n",
      " -1.40469603e+00 -4.85055991e-01  1.34745931e+00 -2.41098816e+00\n",
      "  3.04714598e+00  5.51554433e+00  1.40576995e+00 -6.69303202e-01\n",
      "  2.03161649e+00  3.59072829e+00 -7.41310824e-01 -1.08010301e+00\n",
      " -1.08321386e+00 -1.81703293e+00  2.14097296e-01  2.51106461e+00\n",
      "  6.95944733e-01 -1.11848386e-01 -2.47572756e+00  2.22497579e+00\n",
      " -1.85624703e+00  2.30826815e+00 -8.08608868e-01 -9.99409779e-01\n",
      " -3.76939881e+00 -9.25366928e-01 -1.43157744e+00  1.97322198e+00\n",
      " -8.38300537e-01  2.84363056e+00  9.17712533e-01  2.51137197e+00\n",
      "  6.86864315e-01 -1.35524228e+00  1.35336081e+00  4.07041930e+00\n",
      "  5.24697607e-01 -5.23304030e-01 -8.81021582e-01  1.16501364e+00]\n"
     ]
    }
   ],
   "source": [
    "idx_united_states = word2idx_t['United_States']\n",
    "print(X_embedded[idx_united_states])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='prob87'></a>\n",
    "# 87. 単語の類似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "85で得た単語の意味ベクトルを読み込み，\"United States\"と\"U.S.\"のコサイン類似度を計算せよ．<br>\n",
    "ただし，\"U.S.\"は内部的に\"U.S\"と表現されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_united_states = np.array([X_embedded[idx_united_states]]).astype(np.float32)\n",
    "idx_us = word2idx_t['U.S']\n",
    "vec_us = np.array([X_embedded[idx_us]]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結構よさそう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8260192\n"
     ]
    }
   ],
   "source": [
    "cos_sim = cosine_similarity(vec_united_states, vec_us)[:,0]\n",
    "print(cos_sim[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='prob88'></a>\n",
    "# 88. 類似度の高い単語10件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "85で得た単語の意味ベクトルを読み込み，\"England\"とコサイン類似度が高い10語と，その類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_england = word2idx_t['England']\n",
    "vec_england = np.array([X_embedded[idx_england]]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sliceについて\n",
    "cos_simが2次元配列で返ってくる -> どうせ1列しかないから列のindex0を拾う -> argsortは昇順に返るから\\[::-1\\]で逆順に + England自体は含めたくないからスライスのインデックスは-2から"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = cosine_similarity(X_embedded, vec_england)[:,0]\n",
    "indices_in_sim_order = np.argsort(cos_sim, axis=0)[-2::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5976,   4596,    268, ...,  47021, 216891, 369880])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_in_sim_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'England'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word_t[idx_england]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia 0.6709476764615532\n",
      "Wales 0.631520600202294\n",
      "France 0.6306139123660406\n",
      "Italy 0.6056774585487458\n",
      "Japan 0.603044066132867\n",
      "Germany 0.6018050945339366\n",
      "Scotland 0.5980053601468223\n",
      "Ireland 0.561732085974481\n",
      "Europe 0.5611709314566591\n",
      "India 0.5550795405307398\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "for idx in itertools.islice(indices_in_sim_order,10):\n",
    "    print(idx2word_t[idx], cos_sim[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='prob89'></a>\n",
    "# 89. 加法構成性によるアナロジー"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "85で得た単語の意味ベクトルを読み込み，vec(\"Spain\") - vec(\"Madrid\") + vec(\"Athens\")を計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_spain = word2idx_t['Spain']\n",
    "vec_spain = np.array([X_embedded[idx_spain]]).astype(np.float32)\n",
    "idx_madrid = word2idx_t['Madrid']\n",
    "vec_madrid = np.array([X_embedded[idx_madrid]]).astype(np.float32)\n",
    "idx_athens = word2idx_t['Athens']\n",
    "vec_athens = np.array([X_embedded[idx_athens]]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_comp = vec_spain - vec_madrid + vec_athens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = cosine_similarity(X_embedded, vec_comp)[:,0]\n",
    "indices_in_sim_order = np.argsort(cos_sim, axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spain 0.9000651389763585\n",
      "Sweden 0.7860695296781051\n",
      "Italy 0.7786093381971849\n",
      "Austria 0.7739187200547092\n",
      "Netherlands 0.7362301590224218\n",
      "Denmark 0.7137012568293781\n",
      "Britain 0.7122905688391622\n",
      "Germany 0.6991136996603655\n",
      "Portugal 0.6862837483585569\n",
      "Russia 0.6845829060472209\n"
     ]
    }
   ],
   "source": [
    "for idx in itertools.islice(indices_in_sim_order,10):\n",
    "    print(idx2word_t[idx], cos_sim[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "うーん...\n",
    "<h4>ちなみにGreeceは?</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_greece = word2idx_t['Greece']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "0.6540359601222318\n"
     ]
    }
   ],
   "source": [
    "print(list(indices_in_sim_order).index(idx_greece))\n",
    "print(cos_sim[idx_greece])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
